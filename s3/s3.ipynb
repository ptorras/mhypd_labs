{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11bd60be",
   "metadata": {},
   "source": [
    "# Sesión 3: Inteligencia Artificial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0c725e",
   "metadata": {},
   "source": [
    "Para esta sesión tenéis que responder a las preguntas de las celdas en rojo en la celda adyacente. La entrega se realizará exportando el Jupyter Notebook a pdf y entregándolo en el espacio del Campus Virtual antes de una semana desde el momento de la práctica (22/11/2022 - 23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7cd811",
   "metadata": {
    "pycharm": {
     "name": ""
    }
   },
   "source": [
    "## 3.1 Creación de un modelo de clasificación simple\n",
    "\n",
    "Durante la lección de hoy hemos visto redes neuronales para dos propósitos distintos: modelado de una magnitud continua (regresión) y de una magnitud discreta (Clasificación). En esta práctica vamos a ver como implementar uno de estos modelos de forma muy simple empleando la biblioteca **Keras**.\n",
    "\n",
    "![El logotipo de Keras](https://camo.githubusercontent.com/906e661107a3bc03104ca5d88336d1f4b0e80fdcac65efaf7904041d371c747f/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6b657261732e696f2f696d672f6b657261732d6c6f676f2d323031382d6c617267652d313230302e706e67)\n",
    "\n",
    "Keras es una biblioteca que abstrae prácticamente todos los aspectos computacionales de construir una red neuronal, dejando para el desarrollador solamente definir la estructura del modelo.\n",
    "\n",
    "-------\n",
    "\n",
    "Así pues, **¿cómo se estructura una red neuronal?**\n",
    "\n",
    "Las redes neuronales se construyen insertando capas. Cada capa suele componerse de un conjunto de **parámetros** con los que efectuamos las transformaciones de los datos de entrada para obtener los datos de salida. Como operar con estos parámetros depende del tipo de capas que estemos empleando - las hay para una gran diversidad de tareas!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dcb99c",
   "metadata": {},
   "source": [
    "![Un ejemplo de visualización de una red neuronal](https://miro.medium.com/max/1400/1*Cdim7OfioKQj_rLI9GbNng.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1763e62b",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "Pregunta 1: Buscad algunos de los tipos de capas con parámetros más frecuentes y explicad brevemente para qué se usan: Capa Feedforward/Linear/Fully Connected, Capa Convolucional, Capa Recurrente.\n",
    "</font>\n",
    "\n",
    "Recomendación: Podéis investigar la [documentación de Keras](https://keras.io/api/layers/) o leer algunos artículos: Redes [Convolucionales](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53), [Recurrentes](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks), [Feedforward](https://towardsdatascience.com/feed-forward-neural-networks-how-to-successfully-build-them-in-python-74503409d99a). **Son solo algunos sitios recomendados, no hace falta leerlos en su totalidad ni ceñirse a ellos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c11172",
   "metadata": {},
   "source": [
    "<font color='green'> RESPUESTA AQUÍ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a070f0e5",
   "metadata": {},
   "source": [
    "Después de cada capa con parámetros, se suele insertar una capa de **activación**, cuya tarea es simular las neuronas reales de los cerebros humanos: Despues de recibir los estímulos de entrada de las neuronas precedentes, cada neurona se activa produciendo un valor de salida dependiendo de si se supera un umbral o no. En los gráficos a continuación podéis ver algunas de las funciones de activación más típicas. El eje horizontal describe el valor de **entrada** a la capa de activación y el eje vertical denota el valor de **salida**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc11ab63",
   "metadata": {},
   "source": [
    "![Gráfico de la función ReLU](https://miro.medium.com/max/1400/1*Mk3tjKLpxNKQUnhKSeqRaA.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec3fd0",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "Pregunta 2: La función de activación más frecuente hoy en día es la función ReLU. ¿Qué debe ocurrir para que una neurona produzca un valor de salida positivo? ¿Puede producirse un valor de salida negativo?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c75eb9f",
   "metadata": {},
   "source": [
    "<font color='green'> RESPUESTA AQUÍ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e078839e",
   "metadata": {},
   "source": [
    "La pregunta más natural que se os puede ocurrir es, ¿con qué criterio se estructuran las redes neuronales? ¿Cómo se decide qué capas poner, qué activaciones, etc?\n",
    "\n",
    "Lo cierto es que es un proceso totalmente *arbitrario*. La ciencia de las redes neuronales se basa en el empirismo puro, lo que es una forma bonita de decir que es una cuestión de **prueba y error**. A partir de este conocimiento empírico se puede saber qué tipo de red y con qué numero de parámetros es probable obtener buenos resultados en una tarea determinada: por ejemplo, hay conjuntos de datos muy simples con los que con unos pocos miles de parámetros dispuestos en no mas de siete u ocho capas basta para obtener resultados de cerca del 100% de precisión. Sin embargo, hay tareas (como por ejemplo el COCO dataset que hemos visto durante la clase) para los cuales ni siquiera redes de centenares de millones de parámetros bastan (aunque sí son lo bastante buenas para producir resultados respetables)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4f14f6",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "Pregunta 3: A lo largo de los años se han creado formas de diseñar modelos de forma automatizada para mejorar los resultados y reducir el coste humano de implementar redes neuronales. Definid brevemente en qué consiste NAS (Neural Architecture Search). ¿Cuál es su principal inconveniente?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77858329",
   "metadata": {},
   "source": [
    "<font color='green'> RESPUESTA AQUÍ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ad86b",
   "metadata": {},
   "source": [
    "## 3.2 Manos a la obra\n",
    "\n",
    "Vamos pues a implementar nuestra propia red neuronal de clasificación. Emplearemos uno de los conjuntos de datos predeterminados de Keras, el Dataset MNIST. Es un conjunto de imagenes con números manuscritos del 0 al 9, y el objetivo de clasificación es determinar a qué numero hacen referencia.\n",
    "\n",
    "Este código está basado en el propio [tutorial](https://keras.io/examples/vision/mnist_convnet/) de Keras disponible en su documentación. En general, hagáis lo que hagáis, es **muy** recomendable que leáis los recursos que tiene Keras acerca de Deep Learning, puesto que es una herramienta muy completa con muchísimos ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841166b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 19:03:43.526135: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-13 19:03:43.642578: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-13 19:03:44.309343: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-13 19:03:44.309457: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-13 19:03:44.309462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La clase de salida de la muestra es un 5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def showimg(img):\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Cargamos los datos del dataset MNIST en particiones de entrenamiento y test\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "num_classes = 10\n",
    "n_elms, width, height = x_train.shape\n",
    "\n",
    "# Miramos la forma de los datos. Tenemos 60k imagenes de entrenamiento de un\n",
    "# tamaño de 28 píxeles por 28 con un solo canal. En test tenemos otras 10k imágenes.\n",
    "print(x_train.shape, x_test.shape)\n",
    "\n",
    "# Comprobamos que todo está en orden visualizando una imagen\n",
    "showimg(x_train[0, :][:,:, None])\n",
    "print(f\"La clase de salida de la muestra es un {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b097ef",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "Pregunta 4: Los más observadores habréis visto que hay *dos* conjuntos de datos distintos - train y test. Una de estas mitades de los datos se usa para entrenar el modelo y la otra para evaluar como funciona. No obstante, ¿por qué creéis que se usan particiones distintas? ¿No sería mejor emplear tantos datos como sea posible en entrenar el modelo? Además, a menudo se emplea una tercera partición de validación. Buscad para qué sirve (la vamos a usar aquí también!)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f75e99",
   "metadata": {},
   "source": [
    "<font color='green'> RESPUESTA AQUÍ </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ed2e369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Tenemos que mover los datos enteros al rango -1 a 1\n",
    "x_train = (x_train.astype(\"float32\") - 128) / 255\n",
    "x_test = (x_test.astype(\"float32\") - 128) / 255\n",
    "\n",
    "# Primero usaremos capas feedforward, así que simplemente vamos a\n",
    "# convertir nuestra imagen en un vector\n",
    "x_train_ff = x_train.reshape((-1, width * height))\n",
    "x_test_ff = x_test.reshape((-1, width * height))\n",
    "\n",
    "print(x_train_ff.shape)\n",
    "\n",
    "# Las imágenes tienen que tener una dimensión extra puesto que el modelo\n",
    "# espera 1 o más canales de entrada si usamos redes convolucionales.\n",
    "x_train_conv = np.expand_dims(x_train, -1)\n",
    "x_test_conv = np.expand_dims(x_test, -1)\n",
    "\n",
    "# La salida tiene que disponerse de forma que tengamos un vector\n",
    "# categórico, no una etiqueta única\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45fd1679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 24)                18840     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24)                600       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 12)                300       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 12)                156       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,026\n",
      "Trainable params: 20,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 19:03:45.810338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 19:03:45.903486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 19:03:45.904154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 19:03:45.905896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-13 19:03:45.909474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 19:03:45.910142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 19:03:45.910802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 19:03:46.688161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 19:03:46.688324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 19:03:46.688448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-13 19:03:46.688552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6086 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Es hora de definir el modelo\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=width * height),                        # Capa de entrada al modelo\n",
    "        layers.Dense(width * height // 32, activation=\"relu\"),     # Capa Feedforward 1\n",
    "        layers.Dense(width * height // 32, activation=\"relu\"),     # Capa Feedforward 2\n",
    "        layers.Dense(width * height // 64, activation=\"relu\"),     # Capa Feedforward 3\n",
    "        layers.Dense(width * height // 64, activation=\"relu\"),     # Capa Feedforward 4\n",
    "        \n",
    "        layers.Dense(num_classes, activation=\"softmax\"),           # La salida categorica\n",
    "                                                                   # necesita una capa\n",
    "                                                                   # softmax SIEMPRE\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0101187",
   "metadata": {},
   "source": [
    "En la visualización de arriba tenemos cada una de las capas intermedias del modelo, el tamaño de salida de cada capa y la cantidad de parámetros total.\n",
    "\n",
    "Estamos preparados para entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ff5fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 68/422 [===>..........................] - ETA: 0s - loss: 1.9557 - accuracy: 0.3209"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 19:03:48.041896: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422/422 [==============================] - 2s 2ms/step - loss: 0.8561 - accuracy: 0.7263 - val_loss: 0.3426 - val_accuracy: 0.8977\n",
      "Epoch 2/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.3538 - accuracy: 0.8955 - val_loss: 0.2718 - val_accuracy: 0.9210\n",
      "Epoch 3/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2981 - accuracy: 0.9114 - val_loss: 0.2240 - val_accuracy: 0.9332\n",
      "Epoch 4/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2657 - accuracy: 0.9212 - val_loss: 0.2243 - val_accuracy: 0.9347\n",
      "Epoch 5/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2421 - accuracy: 0.9281 - val_loss: 0.2077 - val_accuracy: 0.9378\n",
      "Epoch 6/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2261 - accuracy: 0.9333 - val_loss: 0.1833 - val_accuracy: 0.9440\n",
      "Epoch 7/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2099 - accuracy: 0.9381 - val_loss: 0.1770 - val_accuracy: 0.9483\n",
      "Epoch 8/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1979 - accuracy: 0.9418 - val_loss: 0.1624 - val_accuracy: 0.9535\n",
      "Epoch 9/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1913 - accuracy: 0.9435 - val_loss: 0.1515 - val_accuracy: 0.9555\n",
      "Epoch 10/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1817 - accuracy: 0.9465 - val_loss: 0.1714 - val_accuracy: 0.9503\n",
      "Epoch 11/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1743 - accuracy: 0.9477 - val_loss: 0.1501 - val_accuracy: 0.9562\n",
      "Epoch 12/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1721 - accuracy: 0.9484 - val_loss: 0.1439 - val_accuracy: 0.9575\n",
      "Epoch 13/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1646 - accuracy: 0.9515 - val_loss: 0.1563 - val_accuracy: 0.9527\n",
      "Epoch 14/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1600 - accuracy: 0.9521 - val_loss: 0.1504 - val_accuracy: 0.9548\n",
      "Epoch 15/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1530 - accuracy: 0.9533 - val_loss: 0.1460 - val_accuracy: 0.9575\n",
      "Epoch 16/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1518 - accuracy: 0.9537 - val_loss: 0.1434 - val_accuracy: 0.9595\n",
      "Epoch 17/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1468 - accuracy: 0.9551 - val_loss: 0.1562 - val_accuracy: 0.9545\n",
      "Epoch 18/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1442 - accuracy: 0.9567 - val_loss: 0.1386 - val_accuracy: 0.9617\n",
      "Epoch 19/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1384 - accuracy: 0.9589 - val_loss: 0.1423 - val_accuracy: 0.9615\n",
      "Epoch 20/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1372 - accuracy: 0.9576 - val_loss: 0.1372 - val_accuracy: 0.9607\n",
      "Epoch 21/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1330 - accuracy: 0.9598 - val_loss: 0.1386 - val_accuracy: 0.9605\n",
      "Epoch 22/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1293 - accuracy: 0.9603 - val_loss: 0.1383 - val_accuracy: 0.9608\n",
      "Epoch 23/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1282 - accuracy: 0.9608 - val_loss: 0.1417 - val_accuracy: 0.9605\n",
      "Epoch 24/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1274 - accuracy: 0.9612 - val_loss: 0.1400 - val_accuracy: 0.9585\n",
      "Epoch 25/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1231 - accuracy: 0.9621 - val_loss: 0.1392 - val_accuracy: 0.9612\n",
      "Epoch 26/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1239 - accuracy: 0.9617 - val_loss: 0.1364 - val_accuracy: 0.9627\n",
      "Epoch 27/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1198 - accuracy: 0.9638 - val_loss: 0.1335 - val_accuracy: 0.9632\n",
      "Epoch 28/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1193 - accuracy: 0.9635 - val_loss: 0.1368 - val_accuracy: 0.9610\n",
      "Epoch 29/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1168 - accuracy: 0.9636 - val_loss: 0.1349 - val_accuracy: 0.9612\n",
      "Epoch 30/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1148 - accuracy: 0.9644 - val_loss: 0.1294 - val_accuracy: 0.9650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe809e85750>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 30\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train_ff, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c42de1",
   "metadata": {},
   "source": [
    "Ahora toca evaluar el modelo con la partición de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63624d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.1519498974084854\n",
      "Test accuracy: 0.955299973487854\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test_ff, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e501ac3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La clase de la imagen es 7\n"
     ]
    }
   ],
   "source": [
    "# Una vez el modelo está entrenado, podéis elegir una imagen cualquiera\n",
    "# del conjunto de test y ver como la clasifica\n",
    "\n",
    "index_sample = 0                      # elegimos la imagen 0 de test\n",
    "test_img = x_test[index_sample]\n",
    "test_class = y_test[index_sample]\n",
    "showimg(test_img[:,:, None])\n",
    "print(f\"La clase de la imagen es {np.argmax(test_class)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f6d5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo predice una clase de 7\n"
     ]
    }
   ],
   "source": [
    "# La pasamos por el modelo, a ver qué dice\n",
    "\n",
    "class_output = model(test_img.reshape(-1, width * height))\n",
    "print(f\"El modelo predice una clase de {np.argmax(class_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239e663c",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "Pregunta 5: Buscad información acerca de las métricas de evaluación. ¿Qué significa tener una accuracy del 75%? ¿Nos interesa que las accuracies de training, validación, y test se parezcan o sean muy distintas? ¿Qué relación tiene la loss con la calidad del modelo? ¿Es una buena forma de evaluar?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c164beb7",
   "metadata": {},
   "source": [
    "<font color='green'> RESPUESTA AQUÍ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07153054",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "Pregunta 6: Fijáos que en el código que extrae la clase, he escrito un ''' np.argmax() '''. ¿Qué hace esta función y qué relación tiene con la función de loss Entropía Cruzada (Cross Entropy)?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2915e",
   "metadata": {},
   "source": [
    "<font color='green'> RESPUESTA AQUÍ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1f949d",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "Pregunta 7: Es vuestro turno. Debéis probar de añadir o quitar capas al modelo, cambiar sus tamaños o sus funciones de activación. Tenéis que dejar al menos 3 versiones del modelo, cada cuál con al menos un cambio significativo y de las cuales al menos una mejore los resultados en la partición de test. Después de cada modelo tenéis que resumir como afecta cada cambio al tiempo de ejecución, a la calidad del modelo, al número de parámetros y a la.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5dca5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 24)                18840     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 24)                600       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 12)                300       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 12)                156       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,026\n",
      "Trainable params: 20,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.9524 - accuracy: 0.6959 - val_loss: 0.4013 - val_accuracy: 0.8923\n",
      "Epoch 2/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.3996 - accuracy: 0.8809 - val_loss: 0.2952 - val_accuracy: 0.9098\n",
      "Epoch 3/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.3161 - accuracy: 0.9059 - val_loss: 0.2388 - val_accuracy: 0.9300\n",
      "Epoch 4/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2759 - accuracy: 0.9170 - val_loss: 0.2109 - val_accuracy: 0.9388\n",
      "Epoch 5/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2529 - accuracy: 0.9245 - val_loss: 0.1905 - val_accuracy: 0.9428\n",
      "Epoch 6/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2315 - accuracy: 0.9303 - val_loss: 0.1810 - val_accuracy: 0.9455\n",
      "Epoch 7/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2164 - accuracy: 0.9345 - val_loss: 0.1722 - val_accuracy: 0.9443\n",
      "Epoch 8/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2063 - accuracy: 0.9375 - val_loss: 0.1732 - val_accuracy: 0.9485\n",
      "Epoch 9/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1960 - accuracy: 0.9398 - val_loss: 0.1833 - val_accuracy: 0.9442\n",
      "Epoch 10/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1858 - accuracy: 0.9433 - val_loss: 0.1581 - val_accuracy: 0.9532\n",
      "Epoch 11/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1772 - accuracy: 0.9464 - val_loss: 0.1508 - val_accuracy: 0.9548\n",
      "Epoch 12/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1734 - accuracy: 0.9470 - val_loss: 0.1452 - val_accuracy: 0.9563\n",
      "Epoch 13/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1666 - accuracy: 0.9496 - val_loss: 0.1481 - val_accuracy: 0.9558\n",
      "Epoch 14/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1612 - accuracy: 0.9516 - val_loss: 0.1493 - val_accuracy: 0.9547\n",
      "Epoch 15/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1564 - accuracy: 0.9526 - val_loss: 0.1414 - val_accuracy: 0.9590\n",
      "Epoch 16/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1518 - accuracy: 0.9536 - val_loss: 0.1371 - val_accuracy: 0.9590\n",
      "Epoch 17/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1488 - accuracy: 0.9545 - val_loss: 0.1422 - val_accuracy: 0.9570\n",
      "Epoch 18/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1443 - accuracy: 0.9562 - val_loss: 0.1384 - val_accuracy: 0.9608\n",
      "Epoch 19/30\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.1419 - accuracy: 0.9567 - val_loss: 0.1312 - val_accuracy: 0.9610\n",
      "Epoch 20/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1366 - accuracy: 0.9588 - val_loss: 0.1321 - val_accuracy: 0.9627\n",
      "Epoch 21/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1364 - accuracy: 0.9591 - val_loss: 0.1359 - val_accuracy: 0.9622\n",
      "Epoch 22/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1345 - accuracy: 0.9589 - val_loss: 0.1305 - val_accuracy: 0.9618\n",
      "Epoch 23/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1297 - accuracy: 0.9600 - val_loss: 0.1308 - val_accuracy: 0.9592\n",
      "Epoch 24/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1309 - accuracy: 0.9594 - val_loss: 0.1319 - val_accuracy: 0.9602\n",
      "Epoch 25/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1273 - accuracy: 0.9608 - val_loss: 0.1377 - val_accuracy: 0.9585\n",
      "Epoch 26/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1237 - accuracy: 0.9622 - val_loss: 0.1259 - val_accuracy: 0.9628\n",
      "Epoch 27/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1214 - accuracy: 0.9630 - val_loss: 0.1355 - val_accuracy: 0.9622\n",
      "Epoch 28/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1186 - accuracy: 0.9634 - val_loss: 0.1232 - val_accuracy: 0.9638\n",
      "Epoch 29/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1174 - accuracy: 0.9638 - val_loss: 0.1439 - val_accuracy: 0.9552\n",
      "Epoch 30/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1146 - accuracy: 0.9642 - val_loss: 0.1384 - val_accuracy: 0.9613\n",
      "Test loss: 0.16601847112178802\n",
      "Test accuracy: 0.9520999789237976\n"
     ]
    }
   ],
   "source": [
    "# MODELO 1 \n",
    "model1 = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=width * height),\n",
    "        \n",
    "        # # # # # # Modificad estas capas\n",
    "        layers.Dense(width * height // 32, activation=\"relu\"),     \n",
    "        layers.Dense(width * height // 32, activation=\"relu\"),     \n",
    "        layers.Dense(width * height // 64, activation=\"relu\"),     \n",
    "        layers.Dense(width * height // 64, activation=\"relu\"), \n",
    "        # # # # # # \n",
    "        \n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model1.summary()\n",
    "\n",
    "model1.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model1.fit(x_train_ff, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "score = model1.evaluate(x_test_ff, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4efb3b3",
   "metadata": {},
   "source": [
    "<font color='green'> VALORACIÓN DEL MODELO 1 AQUÍ </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1558fbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 24)                18840     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 24)                600       \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 12)                300       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 12)                156       \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 10)                130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,026\n",
      "Trainable params: 20,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 1.0414 - accuracy: 0.6562 - val_loss: 0.4608 - val_accuracy: 0.8680\n",
      "Epoch 2/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.4576 - accuracy: 0.8674 - val_loss: 0.3292 - val_accuracy: 0.9043\n",
      "Epoch 3/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.3523 - accuracy: 0.8985 - val_loss: 0.2732 - val_accuracy: 0.9198\n",
      "Epoch 4/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.3007 - accuracy: 0.9117 - val_loss: 0.2588 - val_accuracy: 0.9293\n",
      "Epoch 5/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2712 - accuracy: 0.9200 - val_loss: 0.2030 - val_accuracy: 0.9432\n",
      "Epoch 6/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2524 - accuracy: 0.9248 - val_loss: 0.2115 - val_accuracy: 0.9382\n",
      "Epoch 7/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2417 - accuracy: 0.9279 - val_loss: 0.1848 - val_accuracy: 0.9472\n",
      "Epoch 8/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2300 - accuracy: 0.9310 - val_loss: 0.1810 - val_accuracy: 0.9457\n",
      "Epoch 9/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2195 - accuracy: 0.9349 - val_loss: 0.1774 - val_accuracy: 0.9467\n",
      "Epoch 10/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2126 - accuracy: 0.9369 - val_loss: 0.2116 - val_accuracy: 0.9352\n",
      "Epoch 11/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2076 - accuracy: 0.9382 - val_loss: 0.1694 - val_accuracy: 0.9485\n",
      "Epoch 12/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1976 - accuracy: 0.9418 - val_loss: 0.1754 - val_accuracy: 0.9472\n",
      "Epoch 13/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1929 - accuracy: 0.9430 - val_loss: 0.1685 - val_accuracy: 0.9502\n",
      "Epoch 14/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1878 - accuracy: 0.9441 - val_loss: 0.1598 - val_accuracy: 0.9522\n",
      "Epoch 15/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1828 - accuracy: 0.9451 - val_loss: 0.1741 - val_accuracy: 0.9482\n",
      "Epoch 16/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1798 - accuracy: 0.9463 - val_loss: 0.1716 - val_accuracy: 0.9453\n",
      "Epoch 17/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1739 - accuracy: 0.9480 - val_loss: 0.1520 - val_accuracy: 0.9538\n",
      "Epoch 18/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1728 - accuracy: 0.9483 - val_loss: 0.1487 - val_accuracy: 0.9548\n",
      "Epoch 19/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1674 - accuracy: 0.9489 - val_loss: 0.1722 - val_accuracy: 0.9478\n",
      "Epoch 20/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1649 - accuracy: 0.9498 - val_loss: 0.1564 - val_accuracy: 0.9525\n",
      "Epoch 21/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1631 - accuracy: 0.9515 - val_loss: 0.1513 - val_accuracy: 0.9523\n",
      "Epoch 22/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1604 - accuracy: 0.9519 - val_loss: 0.1607 - val_accuracy: 0.9500\n",
      "Epoch 23/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1561 - accuracy: 0.9519 - val_loss: 0.1543 - val_accuracy: 0.9515\n",
      "Epoch 24/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1578 - accuracy: 0.9514 - val_loss: 0.1462 - val_accuracy: 0.9543\n",
      "Epoch 25/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1539 - accuracy: 0.9533 - val_loss: 0.1594 - val_accuracy: 0.9503\n",
      "Epoch 26/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1508 - accuracy: 0.9543 - val_loss: 0.1432 - val_accuracy: 0.9550\n",
      "Epoch 27/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1477 - accuracy: 0.9549 - val_loss: 0.1446 - val_accuracy: 0.9552\n",
      "Epoch 28/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1460 - accuracy: 0.9554 - val_loss: 0.1547 - val_accuracy: 0.9527\n",
      "Epoch 29/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1441 - accuracy: 0.9561 - val_loss: 0.1447 - val_accuracy: 0.9560\n",
      "Epoch 30/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1429 - accuracy: 0.9559 - val_loss: 0.1391 - val_accuracy: 0.9565\n",
      "Test loss: 0.18553748726844788\n",
      "Test accuracy: 0.944599986076355\n"
     ]
    }
   ],
   "source": [
    "# MODELO 2\n",
    "model2 = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=width * height),\n",
    "        \n",
    "        # # # # # # Modificad estas capas\n",
    "        layers.Dense(width * height // 32, activation=\"relu\"),     \n",
    "        layers.Dense(width * height // 32, activation=\"relu\"),     \n",
    "        layers.Dense(width * height // 64, activation=\"relu\"),     \n",
    "        layers.Dense(width * height // 64, activation=\"relu\"), \n",
    "        # # # # # # \n",
    "        \n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model2.fit(x_train_ff, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "score = model2.evaluate(x_test_ff, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82501455",
   "metadata": {},
   "source": [
    "<font color='green'> VALORACIÓN DEL MODELO 2 AQUÍ </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f12f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_15 (Dense)            (None, 24)                18840     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 24)                600       \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 12)                300       \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 12)                156       \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 10)                130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,026\n",
      "Trainable params: 20,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 1.0009 - accuracy: 0.6559 - val_loss: 0.4468 - val_accuracy: 0.8603\n",
      "Epoch 2/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.4436 - accuracy: 0.8644 - val_loss: 0.3126 - val_accuracy: 0.9062\n",
      "Epoch 3/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.3487 - accuracy: 0.8947 - val_loss: 0.2673 - val_accuracy: 0.9182\n",
      "Epoch 4/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2935 - accuracy: 0.9125 - val_loss: 0.2205 - val_accuracy: 0.9342\n",
      "Epoch 5/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2574 - accuracy: 0.9232 - val_loss: 0.1961 - val_accuracy: 0.9420\n",
      "Epoch 6/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2324 - accuracy: 0.9305 - val_loss: 0.2157 - val_accuracy: 0.9358\n",
      "Epoch 7/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2159 - accuracy: 0.9357 - val_loss: 0.1763 - val_accuracy: 0.9498\n",
      "Epoch 8/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.2030 - accuracy: 0.9381 - val_loss: 0.1591 - val_accuracy: 0.9513\n",
      "Epoch 9/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1901 - accuracy: 0.9426 - val_loss: 0.1760 - val_accuracy: 0.9465\n",
      "Epoch 10/30\n",
      "422/422 [==============================] - 1s 2ms/step - loss: 0.1828 - accuracy: 0.9450 - val_loss: 0.1588 - val_accuracy: 0.9507\n",
      "Epoch 11/30\n",
      "  1/422 [..............................] - ETA: 1s - loss: 0.1445 - accuracy: 0.9688"
     ]
    }
   ],
   "source": [
    "# MODELO 3\n",
    "model3 = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=width * height),\n",
    "        \n",
    "        # # # # # # Modificad estas capas\n",
    "        layers.Dense(width * height // 32, activation=\"relu\"),     \n",
    "        layers.Dense(width * height // 32, activation=\"relu\"),     \n",
    "        layers.Dense(width * height // 64, activation=\"relu\"),     \n",
    "        layers.Dense(width * height // 64, activation=\"relu\"), \n",
    "        # # # # # # \n",
    "        \n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model3.summary()\n",
    "\n",
    "model3.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model3.fit(x_train_ff, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "score = model3.evaluate(x_test_ff, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ce1ab2",
   "metadata": {},
   "source": [
    "<font color='green'> VALORACIÓN DEL MODELO 3 AQUÍ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e50103",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "Pregunta extra: Ahora tratad de implementar un modelo basado en capas convolucionales. Observad las principales diferencias en rendimiento, tiempo de ejecución y número de parámetros con las versiones feedforward. ¿A qué se deben estas diferencias? \n",
    "</font>\n",
    "\n",
    "Basáos en el [ejemplo](https://keras.io/examples/vision/mnist_convnet/) de Keras. Si usáis capas que no hayamos visto en clase o en el lab, investigad acerca de su función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELO 3\n",
    "model_conv = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28, 28, 1)),\n",
    "        \n",
    "        # Codigo aqui\n",
    "        \n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_conv.summary()\n",
    "\n",
    "model_conv.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model_conv.fit(x_train_conv, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "score = model_conv.evaluate(x_test_conv, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
